{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f38be523",
      "metadata": {
        "id": "f38be523"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error,accuracy_score,mean_absolute_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from mlxtend.regressor import StackingRegressor\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.linear_model import ElasticNet, Lasso,Ridge, LinearRegression,HuberRegressor,SGDRegressor,TweedieRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import LinearSVR,SVR\n",
        "from sklearn.metrics import mean_squared_error,explained_variance_score,r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,VotingRegressor\n",
        "import xgboost as xgb\n",
        "import os\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.feature_selection import f_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ae4bd4",
      "metadata": {
        "id": "e9ae4bd4"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,LSTM, GRU,Bidirectional,TimeDistributed, ConvLSTM2D\n",
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.metrics import RootMeanSquaredError,mean_absolute_percentage_error\n",
        "from keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.models import Model\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6L6l18rzuzG",
      "metadata": {
        "id": "_6L6l18rzuzG"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import t\n",
        "from scipy.stats import norm\n",
        "import scipy.stats as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeccf6cd",
      "metadata": {
        "id": "aeccf6cd"
      },
      "outputs": [],
      "source": [
        "# split a univariate time series into patterns\n",
        "def get_Patterns(TSeries, n_inputs,h):\n",
        "    X,y,z = pd.DataFrame(np.zeros((len(TSeries)-n_inputs-h+1,n_inputs))), pd.DataFrame(), pd.DataFrame()\n",
        "    for i in range(len(TSeries)):\n",
        "        # find the end of this pattern\n",
        "        end_ix = i + n_inputs + h - 1\n",
        "        # check if we are beyond the time series\n",
        "        if end_ix > len(TSeries)-1:\n",
        "            break\n",
        "        # gather input and output parts of the pattern\n",
        "        for j in range(n_inputs):\n",
        "            X.loc[i,j]=TSeries.iloc[i+j,0]\n",
        "        i=i+n_inputs\n",
        "        y=y.append(TSeries.iloc[end_ix], ignore_index = True)\n",
        "        sinX=pd.DataFrame(np.sin(X))\n",
        "        cosX=pd.DataFrame(np.cos(X))\n",
        "        squareX=pd.DataFrame(np.power(X,2))\n",
        "        X1=pd.concat([X,sinX,cosX,squareX], axis=1)\n",
        "    return np.array(X1),np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9c68a2",
      "metadata": {
        "id": "aa9c68a2"
      },
      "outputs": [],
      "source": [
        "# originalData should be a Column Vectored DataFrame\n",
        "def minmaxNorm(originalData, lenTrainValidation):\n",
        "    max2norm=max(originalData.iloc[0:lenTrainValidation,0])\n",
        "    min2norm=min(originalData.iloc[0:lenTrainValidation,0])\n",
        "    lenOriginal=len(originalData)\n",
        "    normalizedData=np.zeros(lenOriginal)\n",
        "    normalizedData = []\n",
        "    for i in range (lenOriginal):\n",
        "        normalizedData.append((originalData.iloc[i]-min2norm)/(max2norm-min2norm))\n",
        "    return pd.DataFrame(normalizedData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c481549",
      "metadata": {
        "id": "6c481549"
      },
      "outputs": [],
      "source": [
        "# originalData and forecastedData should be Column Vectored DataFrames\n",
        "def minmaxDeNorm( originalData, forecastedData, lenTrainValidation):\n",
        "    max2norm=max(originalData.iloc[0:lenTrainValidation,0])\n",
        "    min2norm=min(originalData.iloc[0:lenTrainValidation,0])\n",
        "    lenOriginal=len(originalData)\n",
        "    denormalizedData=[]\n",
        "    for i in range (lenOriginal):\n",
        "        denormalizedData.append((forecastedData.iloc[i]*(max2norm-min2norm))+min2norm)\n",
        "    return pd.DataFrame(denormalizedData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5142a09",
      "metadata": {
        "id": "f5142a09"
      },
      "outputs": [],
      "source": [
        "# Timeseries_Data and forecasted_value should be Column Vectored DataFrames\n",
        "def findRMSE( Timeseries_Data, forecasted_value,lenTrainValidation):\n",
        "    l=Timeseries_Data.shape[0]\n",
        "    lenTest=l-lenTrainValidation\n",
        "    trainRMSE=0;\n",
        "    for i in range (lenTrainValidation):\n",
        "        trainRMSE=trainRMSE+np.power((forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0]),2)\n",
        "    trainRMSE=np.sqrt(trainRMSE/lenTrainValidation)\n",
        "\n",
        "    testRMSE=0;\n",
        "    for i in range (lenTrainValidation,l,1):\n",
        "        testRMSE=testRMSE+np.power((forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0]),2)\n",
        "    testRMSE=np.sqrt(testRMSE/lenTest)\n",
        "    return trainRMSE, testRMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9147158",
      "metadata": {
        "id": "c9147158"
      },
      "outputs": [],
      "source": [
        "# Timeseries_Data and forecasted_value should be Column Vectored DataFrames\n",
        "def findSMAPE( Timeseries_Data, forecasted_value,lenTrainValidation):\n",
        "    l=Timeseries_Data.shape[0]\n",
        "    lenTest=l-lenTrainValidation\n",
        "    trainSMAPE=0;\n",
        "    for i in range (lenTrainValidation):\n",
        "        trainSMAPE=trainSMAPE+(np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])/((np.abs(forecasted_value.iloc[i,0])+np.abs(Timeseries_Data.iloc[i,0]))/2))\n",
        "\n",
        "    trainSMAPE=(trainSMAPE/(lenTrainValidation))*100;\n",
        "\n",
        "    testSMAPE=0;\n",
        "    for i in range (lenTrainValidation,l,1):\n",
        "        testSMAPE=testSMAPE+(np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])/((np.abs(forecasted_value.iloc[i,0])+np.abs(Timeseries_Data.iloc[i,0]))/2))\n",
        "\n",
        "    testSMAPE=(testSMAPE/lenTest)*100;\n",
        "    return trainSMAPE, testSMAPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f22d5fb",
      "metadata": {
        "id": "7f22d5fb"
      },
      "outputs": [],
      "source": [
        "# Timeseries_Data and forecasted_value should be Column Vectored DataFrames\n",
        "def findMAE( Timeseries_Data, forecasted_value,lenTrainValidation):\n",
        "    l=Timeseries_Data.shape[0]\n",
        "    lenTest=l-lenTrainValidation\n",
        "    trainMAE=0;\n",
        "    for i in range (lenTrainValidation):\n",
        "        trainMAE=trainMAE+np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])\n",
        "    trainMAE=(trainMAE/(lenTrainValidation));\n",
        "\n",
        "    testMAE=0;\n",
        "    for i in range (lenTrainValidation,l,1):\n",
        "        testMAE=testMAE+np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])\n",
        "    testMAE=(testMAE/lenTest);\n",
        "    return trainMAE, testMAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db78d19c",
      "metadata": {
        "id": "db78d19c"
      },
      "outputs": [],
      "source": [
        "# Timeseries_Data and forecasted_value should be Column Vectored DataFrames\n",
        "def findMASE( Timeseries_Data, forecasted_value,lenTrainValidation):\n",
        "    l=Timeseries_Data.shape[0]\n",
        "    lenTest=l-lenTrainValidation\n",
        "    trainMASE=0;\n",
        "    T1=Timeseries_Data.iloc[0:lenTrainValidation,0]\n",
        "    T2=T1.diff()\n",
        "    T2=T2.drop(0)\n",
        "    T2=np.abs(T2)\n",
        "    M=np.mean(T2)\n",
        "    for i in range (lenTrainValidation):\n",
        "        trainMASE=trainMASE+(np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])/M)\n",
        "\n",
        "    trainMASE=(trainMASE/(lenTrainValidation))\n",
        "\n",
        "    testMASE=0\n",
        "    T1=Timeseries_Data.iloc[lenTrainValidation:l,0]\n",
        "    T2=T1.diff()\n",
        "    T2=T2.drop(lenTrainValidation)\n",
        "    T2=np.abs(T2)\n",
        "    M=np.mean(T2)\n",
        "    for i in range (lenTrainValidation,l,1):\n",
        "        testMASE=testMASE+(np.abs(forecasted_value.iloc[i,0]-Timeseries_Data.iloc[i,0])/M)\n",
        "    testMASE=(testMASE/lenTest)\n",
        "    return trainMASE,testMASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1af1cf2",
      "metadata": {
        "id": "a1af1cf2"
      },
      "outputs": [],
      "source": [
        "def Find_Fitness(x,y,lenValid,lenTest,model):\n",
        "    NOP=y.shape[0]\n",
        "    lenTrain=NOP-lenValid-lenTest\n",
        "    xTrain=x[0:(lenTrain+lenValid),:]\n",
        "    xValid=x[lenTrain:(lenTrain+lenValid),:]\n",
        "    xTest=x[(lenTrain+lenValid):NOP,:]\n",
        "    yTrain=y[0:(lenTrain+lenValid),0]\n",
        "    yValid=y[lenTrain:(lenTrain+lenValid),0]\n",
        "    yTest=y[(lenTrain+lenValid):NOP,0]\n",
        "    model.fit(xTrain, yTrain)\n",
        "    yhatNorm=model.predict(x).flatten().reshape(x.shape[0],1)\n",
        "    return pd.DataFrame(yhatNorm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QH9TFR3DxcKD",
      "metadata": {
        "id": "QH9TFR3DxcKD"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_gaussian(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.norm.interval(confidence=confidenceLevel, loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData+confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KZEIoJL-8dnf",
      "metadata": {
        "id": "KZEIoJL-8dnf"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_t_location_scale(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.t.interval(confidence=confidenceLevel, df=error.shape[0]-1,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData+confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NJ8QZa558eqk",
      "metadata": {
        "id": "NJ8QZa558eqk"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_beta(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    a, b = 2.31, 0.627\n",
        "    confidenceInterval95 = st.beta.interval(confidence=confidenceLevel, a=a,b=b,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData-confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XdTLN-Bz8jHB",
      "metadata": {
        "id": "XdTLN-Bz8jHB"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_cauchy(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.cauchy.interval(confidence=confidenceLevel, loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData+confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-bUyVT2Z8phx",
      "metadata": {
        "id": "-bUyVT2Z8phx"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_gamma(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    a=1.99\n",
        "    confidenceInterval95 = st.gamma.interval(confidence=confidenceLevel, a=a,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData-confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5AanNyr8tML",
      "metadata": {
        "id": "y5AanNyr8tML"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_rayleigh(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.rayleigh.interval(confidence=confidenceLevel,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData-confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_2HDgnoJ8u3G",
      "metadata": {
        "id": "_2HDgnoJ8u3G"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_uniform(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.uniform.interval(confidence=confidenceLevel,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData-confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hvte2I8Z8zDX",
      "metadata": {
        "id": "hvte2I8Z8zDX"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_expon(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.expon.interval(confidence=confidenceLevel,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData-confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QkONb89M83S3",
      "metadata": {
        "id": "QkONb89M83S3"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_laplace(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.laplace.interval(confidence=confidenceLevel,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData+confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eqsTmHAO86dO",
      "metadata": {
        "id": "eqsTmHAO86dO"
      },
      "outputs": [],
      "source": [
        "def find_confidence_interval_maxwell(confidenceLevel,sampleMean,sampleStandardError,error,ForecastedData):\n",
        "    degrees_freedom = len(error)-1  #degree of freedom = sample size-1\n",
        "    #sampleMean = np.mean(error)    #sample mean\n",
        "    #sampleStandardError = st.sem(error)  #sample standard error\n",
        "    confidenceInterval95 = st.maxwell.interval(confidence=confidenceLevel,loc=sampleMean, scale=sampleStandardError)\n",
        "    lower95=pd.DataFrame(ForecastedData-confidenceInterval95[0])\n",
        "    upper95=pd.DataFrame(ForecastedData+confidenceInterval95[1])\n",
        "    df = pd.concat([lower95, upper95], axis = 1)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RZbnGTH1xig1",
      "metadata": {
        "id": "RZbnGTH1xig1"
      },
      "outputs": [],
      "source": [
        "def find_Prediction_Interval_Evaluation(pi,Original,Forecasted,ci):\n",
        "    # Computation of Prediction Interval Coverage Probability (PICP)\n",
        "    count=0\n",
        "    cc=pd.DataFrame()\n",
        "    for i in range(0,Original.shape[0],1):\n",
        "        dat=Original.iloc[i,0]\n",
        "        if dat >= pi.iloc[i,0] and dat <= pi.iloc[i,1]:\n",
        "            count+=1\n",
        "    PICP=(1/Original.shape[0])*count*100\n",
        "\n",
        "    # Computation of Prediction Interval Normalized Average Width (PINAW)\n",
        "    sum=0\n",
        "    for i in range(0,pi.shape[0],1):\n",
        "        sum+=pi.iloc[i,1]-pi.iloc[i,0]\n",
        "    A=np.max(Original)-np.min(Original)\n",
        "    PINAW=(1/(A*pi.shape[0]))*sum\n",
        "\n",
        "    # Computation of Accumulated Width Deviation (AWD)\n",
        "    sum=0\n",
        "    for i in range(0,Original.shape[0],1):\n",
        "        dat=Original.iloc[i,0]\n",
        "        if dat >= pi.iloc[i,0] and dat <= pi.iloc[i,1]:\n",
        "            sum+=0\n",
        "        elif dat < pi.iloc[i,0]:\n",
        "            sum+=(pi.iloc[i,0]-dat)/(pi.iloc[i,1]-pi.iloc[i,0])\n",
        "        else:\n",
        "            sum+=(dat-pi.iloc[i,1])/(pi.iloc[i,1]-pi.iloc[i,0])\n",
        "    AWD=(1/Original.shape[0])*sum\n",
        "\n",
        "    # Computation of Average Coverage Error (ACE)=PICP-PINC\n",
        "    ACE=PICP-ci\n",
        "\n",
        "    return PICP, PINAW, AWD,ACE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t9C-THA57EI5",
      "metadata": {
        "id": "t9C-THA57EI5"
      },
      "outputs": [],
      "source": [
        "def gaussian(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Gaussian Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_gaussian(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_gaussian(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_gaussian(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_gaussian(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_gaussian(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=file_name)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Gaussian_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AwiipbHV9pd7",
      "metadata": {
        "id": "AwiipbHV9pd7"
      },
      "outputs": [],
      "source": [
        "def t_location_scale(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with t_location_scale Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_t_location_scale(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_t_location_scale(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_t_location_scale(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_t_location_scale(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_t_location_scale(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"t_Location_Scale_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GKfown2y9slz",
      "metadata": {
        "id": "GKfown2y9slz"
      },
      "outputs": [],
      "source": [
        "def Beta(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Beta Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_beta(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_beta(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_beta(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_beta(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_beta(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Beta_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FSmHSG3K9vZG",
      "metadata": {
        "id": "FSmHSG3K9vZG"
      },
      "outputs": [],
      "source": [
        "def Cauchy(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Cauchy Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_cauchy(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_cauchy(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_cauchy(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_cauchy(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_cauchy(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Cauchy_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0hiMxwCE9yrj",
      "metadata": {
        "id": "0hiMxwCE9yrj"
      },
      "outputs": [],
      "source": [
        "def Gamma(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Gamma Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_gamma(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_gamma(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_gamma(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_gamma(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_gamma(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Gamma_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KTJIFBmq91MK",
      "metadata": {
        "id": "KTJIFBmq91MK"
      },
      "outputs": [],
      "source": [
        "def Rayleigh(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Rayleigh Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_rayleigh(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_rayleigh(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_rayleigh(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_rayleigh(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_rayleigh(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Rayleigh_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XiMBOyM794rz",
      "metadata": {
        "id": "XiMBOyM794rz"
      },
      "outputs": [],
      "source": [
        "def Uniform(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Uniform Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_uniform(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_uniform(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_uniform(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_uniform(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_uniform(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Uniform_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fH7wRLJ697lo",
      "metadata": {
        "id": "fH7wRLJ697lo"
      },
      "outputs": [],
      "source": [
        "def Exponential(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Exponential Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_expon(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_expon(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_expon(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_expon(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_expon(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Exponential_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4mf9uOHy9-Cv",
      "metadata": {
        "id": "4mf9uOHy9-Cv"
      },
      "outputs": [],
      "source": [
        "def Laplace(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Laplace Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_laplace(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_laplace(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_laplace(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_laplace(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_laplace(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Laplace_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jt3mhhFW-AhZ",
      "metadata": {
        "id": "Jt3mhhFW-AhZ"
      },
      "outputs": [],
      "source": [
        "def Maxwell(Original,ForecastedData,error,filename,modelname,Scale):\n",
        "    #Calculate different confidence intervals with Maxwell Distribution\n",
        "    location=0\n",
        "    conf95=find_confidence_interval_maxwell(0.95,location,Scale,error,ForecastedData)\n",
        "    conf90=find_confidence_interval_maxwell(0.90,location,Scale,error,ForecastedData)\n",
        "    conf80=find_confidence_interval_maxwell(0.80,location,Scale,error,ForecastedData)\n",
        "    conf70=find_confidence_interval_maxwell(0.70,location,Scale,error,ForecastedData)\n",
        "    conf60=find_confidence_interval_maxwell(0.60,location,Scale,error,ForecastedData)\n",
        "    IF = pd.concat([conf95, conf90, conf80, conf70, conf60], axis=1)\n",
        "    fig, ax = plt.subplots(1,1)\n",
        "    fig.set_figheight(5)\n",
        "    fig.set_figwidth(10)\n",
        "    ax.fill_between(conf95.index,conf95.iloc[:,0],conf95.iloc[:,1], color='#ff0000',label='95%')\n",
        "    ax.fill_between(conf90.index,conf90.iloc[:,0],conf90.iloc[:,1], color='#cf0000',label='90%')\n",
        "    ax.fill_between(conf80.index,conf80.iloc[:,0],conf80.iloc[:,1], color='#af0000',label='80%')\n",
        "    ax.fill_between(conf90.index,conf70.iloc[:,0],conf70.iloc[:,1], color='#8f0000',label='70%')\n",
        "    ax.fill_between(conf80.index,conf60.iloc[:,0],conf60.iloc[:,1], color='#6f0000',label='60%')\n",
        "    ax.plot(ForecastedData, linewidth=3,label='Forecasts')\n",
        "    ax.plot(Original, linewidth=3,label='True Values',color='black', ls=':')\n",
        "    ax.legend(loc=\"best\")\n",
        "    ax.set(xlabel='Time / Month', ylabel=filename)\n",
        "    PIEvaluation=pd.DataFrame(np.zeros((5,4)))\n",
        "    PIEvaluation.iloc[0,0], PIEvaluation.iloc[0,1], PIEvaluation.iloc[0,2], PIEvaluation.iloc[0,3]=find_Prediction_Interval_Evaluation(conf95,Original,ForecastedData,95)\n",
        "    PIEvaluation.iloc[1,0], PIEvaluation.iloc[1,1], PIEvaluation.iloc[1,2], PIEvaluation.iloc[1,3]=find_Prediction_Interval_Evaluation(conf90,Original,ForecastedData,90)\n",
        "    PIEvaluation.iloc[2,0], PIEvaluation.iloc[2,1], PIEvaluation.iloc[2,2], PIEvaluation.iloc[2,3]=find_Prediction_Interval_Evaluation(conf80,Original,ForecastedData,80)\n",
        "    PIEvaluation.iloc[3,0], PIEvaluation.iloc[3,1], PIEvaluation.iloc[3,2], PIEvaluation.iloc[3,3]=find_Prediction_Interval_Evaluation(conf70,Original,ForecastedData,70)\n",
        "    PIEvaluation.iloc[4,0], PIEvaluation.iloc[4,1], PIEvaluation.iloc[4,2], PIEvaluation.iloc[4,3]=find_Prediction_Interval_Evaluation(conf60,Original,ForecastedData,60)\n",
        "    img_name=filename+\"_\"+modelname+\"Maxwell_Distribution_Prediction_Interval.png\"\n",
        "    plt.savefig(img_name)\n",
        "    return IF,PIEvaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4jFZhzq5jViW",
      "metadata": {
        "id": "4jFZhzq5jViW"
      },
      "outputs": [],
      "source": [
        "#Deterministic Forecasting\n",
        "for k in range(0,10,1):\n",
        "    filename=''\n",
        "    LagLength=1\n",
        "    #0-3 leptokurtic TSD\n",
        "    if k==0:\n",
        "        filename='Alcohol.csv'\n",
        "        LagLength=2\n",
        "    elif k==1:\n",
        "        filename='lynx.csv'\n",
        "        LagLength=10\n",
        "    elif k==2:\n",
        "        filename='River.csv'\n",
        "        LagLength=12\n",
        "    elif k==3:\n",
        "        filename='stock.csv'\n",
        "        LagLength=2\n",
        "    #4-7 Platykurtic TSD\n",
        "    elif k==4:\n",
        "        filename='NewWorkWater.csv'\n",
        "        LagLength=2\n",
        "    elif k==5:\n",
        "        filename='PlasticContainerDemand.csv'\n",
        "        LagLength=2\n",
        "    elif k==6:\n",
        "        filename='temp.csv'\n",
        "        LagLength=12\n",
        "    elif k==7:\n",
        "        filename='ToothpasteMarketShare.csv'\n",
        "        LagLength=2\n",
        "    #Mesokurtic\n",
        "    elif k==8:\n",
        "        filename='chicken.csv'\n",
        "        LagLength=12\n",
        "    elif k==9:\n",
        "        filename='IntTraUK.csv'\n",
        "        LagLength=12\n",
        "    #Read the Time Series Dataset\n",
        "    Timeseries_Data=pd.read_csv(filename,header=None)\n",
        "    h=1\n",
        "    lt=Timeseries_Data.shape[0]\n",
        "    lenTrain=int(round(lt*0.6))\n",
        "    lenValidation=int(round(lt*0.2))\n",
        "    lenTest=int(lt-lenTrain-lenValidation)\n",
        "    # NORMALIZE THE DATA\n",
        "    normalizedData=minmaxNorm(Timeseries_Data,lenTrain+lenValidation);\n",
        "    # Transform the Time Series into Patterns Using Sliding Window\n",
        "    X, y = get_Patterns(normalizedData, LagLength, h)\n",
        "    Deterministic_MeanAccuracyT=pd.DataFrame()\n",
        "    Deterministic_StdAccuracyT=pd.DataFrame()\n",
        "    file1='./Deterministic_'+str(filename[:-4])+\"_Accuracy.xlsx\"\n",
        "    file2='./Deterministic_'+str(filename[:-4])+\"_Forecasts.xlsx\"\n",
        "    Deterministic_Forecasts=pd.DataFrame()\n",
        "    Deterministic_Accuracy=pd.DataFrame()\n",
        "    for j in range(0,6,1):\n",
        "        # print('Model:::::::::::::::::::::::::::::::',j)\n",
        "        if j==0:\n",
        "            model=LinearRegression()\n",
        "            name='1.LinearRegression'\n",
        "        elif j==1:\n",
        "            model=Lasso()\n",
        "            name='2.Lasso'\n",
        "        elif j==2:\n",
        "            model=Ridge()\n",
        "            name='3.Ridge'\n",
        "        elif j==3:\n",
        "            model=ElasticNet()\n",
        "            name='4.ElasticNet'\n",
        "        elif j==4:\n",
        "            model=HuberRegressor()\n",
        "            name='5.HuberRegressor'\n",
        "        elif j==5:\n",
        "            model=LinearSVR()\n",
        "            name='6.LinearSVR'\n",
        "        else:\n",
        "            print('Completed.....................')\n",
        "        ynorm1=Find_Fitness(X,y,lenValidation,lenTest,model)\n",
        "        ynorm=pd.DataFrame(normalizedData.iloc[0:LagLength,0])\n",
        "        ynorm=ynorm.append(ynorm1,ignore_index = True)\n",
        "        yhat=minmaxDeNorm(Timeseries_Data, ynorm, lenTrain+lenValidation)\n",
        "\n",
        "        Deterministic_Accuracy.loc[j,0],Deterministic_Accuracy.loc[j,1]=findRMSE( Timeseries_Data,yhat,lenTrain+lenValidation)\n",
        "        Deterministic_Accuracy.loc[j,2],Deterministic_Accuracy.loc[j,3]=findSMAPE( Timeseries_Data,yhat,lenTrain+lenValidation)\n",
        "        Deterministic_Accuracy.loc[j,4],Deterministic_Accuracy.loc[j,5]=findMAE( Timeseries_Data,yhat,lenTrain+lenValidation)\n",
        "        Deterministic_Accuracy.loc[j,6],Deterministic_Accuracy.loc[j,7]=findMASE( Timeseries_Data,yhat,lenTrain+lenValidation)\n",
        "        Deterministic_Forecasts=Deterministic_Forecasts.append(yhat.T,ignore_index = True)\n",
        "    Deterministic_Accuracy.to_excel(file1,sheet_name='Accuracy',index=False)\n",
        "    Deterministic_Forecasts.T.to_excel(file2,sheet_name='Forecasts',index=False)\n",
        "    Deterministic_MeanAccuracy=pd.DataFrame(np.mean(Deterministic_Accuracy))\n",
        "    Deterministic_MeanAccuracyT=Deterministic_MeanAccuracyT.append(Deterministic_MeanAccuracy.T, ignore_index = True)\n",
        "    Deterministic_StdAccuracy=pd.DataFrame(np.std(Deterministic_Accuracy))\n",
        "    Deterministic_StdAccuracyT=Deterministic_StdAccuracyT.append(Deterministic_StdAccuracy.T, ignore_index = True)\n",
        "    del Deterministic_Accuracy\n",
        "    del Deterministic_Forecasts\n",
        "    del Deterministic_MeanAccuracy\n",
        "    del Deterministic_StdAccuracy\n",
        "Deterministic_MeanAccuracyT.to_excel('All_Model_Point_Mean_Accuracy.xlsx',sheet_name='All_Model_Point_Accuracy',index=False)\n",
        "del Deterministic_MeanAccuracyT\n",
        "Deterministic_StdAccuracyT.to_excel('All_Model_Point_Stdev_Accuracy.xlsx',sheet_name='All_Model_Point_Stdev_Accuracy',index=False)\n",
        "del Deterministic_StdAccuracyT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e6a14b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b5e6a14b",
        "outputId": "ed95e69e-ce99-4813-d23a-971e87b23d63"
      },
      "outputs": [],
      "source": [
        "#Interval Forecasting\n",
        "for k in range(0,10,1):\n",
        "    filename='';\n",
        "    Forecasts=pd.DataFrame()\n",
        "    Accuracy=pd.DataFrame()\n",
        "    #0-3 leptokurtic TSD\n",
        "    if k==0:\n",
        "        filename='Alcohol.csv'\n",
        "        model=LinearSVR()\n",
        "        LagLength=2\n",
        "        Scale=1\n",
        "    elif k==1:\n",
        "        filename='lynx.csv'\n",
        "        model=Ridge()\n",
        "        LagLength=10\n",
        "        Scale=100\n",
        "    elif k==2:\n",
        "        filename='River.csv'\n",
        "        model=HuberRegressor()\n",
        "        LagLength=12\n",
        "        Scale=1\n",
        "    elif k==3:\n",
        "        filename='stock.csv'\n",
        "        model=Ridge()\n",
        "        LagLength=2\n",
        "        Scale=5\n",
        "    #4-7 Platykurtic TSD\n",
        "    elif k==4:\n",
        "        filename='NewWorkWater.csv'\n",
        "        model=HuberRegressor()\n",
        "        LagLength=2\n",
        "        Scale=5\n",
        "    elif k==5:\n",
        "        filename='PlasticContainerDemand.csv'\n",
        "        model=HuberRegressor()\n",
        "        LagLength=2\n",
        "        Scale=100\n",
        "    elif k==6:\n",
        "        filename='temp.csv'\n",
        "        model=HuberRegressor()\n",
        "        LagLength=12\n",
        "        Scale=5\n",
        "    elif k==7:\n",
        "        filename='ToothpasteMarketShare.csv'\n",
        "        model=HuberRegressor()\n",
        "        LagLength=2\n",
        "        Scale=0.5\n",
        "    #Mesokurtic\n",
        "    elif k==8:\n",
        "        filename='chicken.csv'\n",
        "        model=HuberRegressor()\n",
        "        LagLength=12\n",
        "        Scale=200\n",
        "    elif k==9:\n",
        "        filename='IntTraUK.csv'\n",
        "        model=LinearRegression()\n",
        "        LagLength=12\n",
        "        Scale=200\n",
        "    #Read the Time Series Dataset\n",
        "    Original=pd.read_csv(filename,header=None)\n",
        "    h=1\n",
        "    lt=Original.shape[0]\n",
        "    lenTrain=int(round(lt*0.6))\n",
        "    lenValidation=int(round(lt*0.2))\n",
        "    lenTest=int(lt-lenTrain-lenValidation)\n",
        "    # NORMALIZE THE DATA\n",
        "    normalizedData=minmaxNorm(Original,lenTrain+lenValidation);\n",
        "    # Transform the Time Series into Patterns Using Sliding Window\n",
        "    X, y = get_Patterns(normalizedData, LagLength, h)\n",
        "    file1='./Probabilistic_'+str(filename[:-4])+\"_Accuracy.xlsx\"\n",
        "    file2='./Probabilistic_'+str(filename[:-4])+\"_Forecasts.xlsx\"\n",
        "    MeanAccuracyT=pd.DataFrame()\n",
        "    StdAccuracyT=pd.DataFrame()\n",
        "    file_name = filename[:-4]\n",
        "    ynorm1=Find_Fitness(X,y,lenValidation,lenTest,model)\n",
        "    ynorm=pd.DataFrame(normalizedData.iloc[0:LagLength,0])\n",
        "    ynorm=ynorm.append(ynorm1,ignore_index = True)\n",
        "    ForecastedData=minmaxDeNorm(Original, ynorm, lenTrain+lenValidation)\n",
        "    error=Original-ForecastedData\n",
        "    PIE=pd.DataFrame(np.zeros((5,4)))\n",
        "    for d in range(10):\n",
        "        if d==0:\n",
        "            IF,PIE=gaussian(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==1:\n",
        "            IF,PIE=t_location_scale(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==2:\n",
        "            IF,PIE=Beta(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==3:\n",
        "            IF,PIE=Cauchy(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==4:\n",
        "            IF,PIE=Gamma(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==5:\n",
        "            IF,PIE=Rayleigh(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==6:\n",
        "            IF,PIE=Uniform(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==7:\n",
        "            IF,PIE=Exponential(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==8:\n",
        "            IF,PIE=Laplace(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        elif d==9:\n",
        "            IF,PIE=Maxwell(Original,ForecastedData,error,file_name,name[2:],Scale)\n",
        "        else:\n",
        "            print('Completed.....................')\n",
        "        t=-1\n",
        "        for q in range(20):\n",
        "            if(q%5==0):\n",
        "                t+=1\n",
        "            Accuracy.loc[d,q]=PIE.loc[t,q%4]\n",
        "        Forecasts=pd.concat([Forecasts,IF],axis=1)\n",
        "    Accuracy.to_excel(file1,sheet_name='Accuracy',index=False)\n",
        "    Forecasts.T.to_excel(file2,sheet_name='Forecasts',index=False)\n",
        "    MeanAccuracy=pd.DataFrame(np.mean(Accuracy))\n",
        "    MeanAccuracyT=MeanAccuracyT.append(MeanAccuracy.T, ignore_index = True)\n",
        "    StdAccuracy=pd.DataFrame(np.std(Accuracy))\n",
        "    StdAccuracyT=StdAccuracyT.append(StdAccuracy.T, ignore_index = True)\n",
        "    del Accuracy\n",
        "    # del Forecasts\n",
        "    del MeanAccuracy\n",
        "    del StdAccuracy\n",
        "MeanAccuracyT.to_excel('All_Model_Mean_Accuracy.xlsx',sheet_name='All_Model_Accuracy',index=False)\n",
        "del MeanAccuracyT\n",
        "StdAccuracyT.to_excel('All_Model_Stdev_Accuracy.xlsx',sheet_name='All_Model_Stdev_Accuracy',index=False)\n",
        "del StdAccuracyT\n",
        "# print(Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CKETz-Q-JAWe",
      "metadata": {
        "id": "CKETz-Q-JAWe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
